{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Networks\n",
    "By generating more similar data, we can increase the data size to reduce our training variance error, i.e. over fitting problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, multiply, BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import Reshape, Dense, Dropout, Flatten\n",
    "from keras.layers import ELU, PReLU, LeakyReLU\n",
    "from keras.layers.convolutional import Conv2D, UpSampling2D\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build generator and discrimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(latent_dim,data_dim):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(16, input_dim=latent_dim))\n",
    "    \n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(32, input_dim=latent_dim))\n",
    "    \n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(data_dim,activation='tanh'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = build_generator(latent_dim=10,data_dim=29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(data_dim,num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(31,input_dim=data_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(16,input_dim=data_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.summary()\n",
    "    img = Input(shape=(data_dim,))\n",
    "    features = model(img)\n",
    "    valid = Dense(1, activation=\"sigmoid\")(features)\n",
    "    label = Dense(num_classes+1, activation=\"softmax\")(features)\n",
    "    return Model(img, [valid, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = build_discriminator(data_dim=29,num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(0.0002, 0.5)\n",
    "discriminator.compile(loss=['binary_crossentropy', 'categorical_crossentropy'],\n",
    "    loss_weights=[0.5, 0.5],\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = Input(shape=(10,))\n",
    "img = generator(noise)\n",
    "discriminator.trainable = False\n",
    "valid,_ = discriminator(img)\n",
    "combined = Model(noise , valid)\n",
    "combined.compile(loss=['binary_crossentropy'],\n",
    "    optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train,y_train,\n",
    "          X_test,y_test,\n",
    "          generator,discriminator,\n",
    "          combined,\n",
    "          num_classes,\n",
    "          epochs, \n",
    "          batch_size=128):\n",
    "    \n",
    "    f1_progress = []\n",
    "    half_batch = int(batch_size / 2)\n",
    "\n",
    "    noise_until = epochs\n",
    "\n",
    "    # Class weights:\n",
    "    # To balance the difference in occurences of digit class labels.\n",
    "    # 50% of labels that the discriminator trains on are 'fake'.\n",
    "    # Weight = 1 / frequency\n",
    "    cw1 = {0: 1, 1: 1}\n",
    "    cw2 = {i: num_classes / half_batch for i in range(num_classes)}\n",
    "    cw2[num_classes] = 1 / half_batch\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        # Select a random half batch of data\n",
    "        idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "        imgs = X_train[idx]\n",
    "\n",
    "        # Sample noise and generate a half batch of new images\n",
    "        noise = np.random.normal(0, 1, (half_batch, 10))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "\n",
    "        valid = np.ones((half_batch, 1))\n",
    "        fake = np.zeros((half_batch, 1))\n",
    "\n",
    "        #labels = to_categorical(y_train[idx], num_classes=num_classes+1)\n",
    "        #fake_labels = to_categorical(np.full((half_batch, 1), num_classes), num_classes=num_classes+1)\n",
    "\n",
    "        # Train the discriminator\n",
    "        d_loss_real = discriminator.train_on_batch(imgs, [valid, noise], class_weight=[cw1, cw2])\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs, [fake, noise], class_weight=[cw1, cw2])\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "\n",
    "        noise = np.random.normal(0, 1, (batch_size, 10))\n",
    "        validity = np.ones((batch_size, 1))\n",
    "\n",
    "        # Train the generator\n",
    "        g_loss = combined.train_on_batch(noise, validity, class_weight=[cw1, cw2])\n",
    "\n",
    "        # Plot the progress\n",
    "        print (\"%d [D loss: %f, acc: %.2f%%, op_acc: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[3], 100*d_loss[4], g_loss))\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            _,y_pred = discriminator.predict(X_test,batch_size=batch_size)\n",
    "            #print(y_pred.shape)\n",
    "            y_pred = np.argmax(y_pred[:,:-1],axis=1)\n",
    "            \n",
    "            f1 = f1_score(y_test,y_pred)\n",
    "            print('Epoch: {}, F1: {:.5f}, F1P: {}'.format(epoch,f1,len(f1_progress)))\n",
    "            f1_progress.append(f1)\n",
    "            \n",
    "    return f1_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../Data/creditcard.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "X = df.drop(['Class'], axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state = 43, shuffle= False)\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idx = np.random.randint(0, X_train.shape[0], 200)\n",
    "#y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_p = train(X_train,y_train,\n",
    "             X_test,y_test,\n",
    "             generator,discriminator,\n",
    "             combined,\n",
    "             num_classes=2,\n",
    "             epochs=5000, \n",
    "             batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('anomaly')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "398cd56811f9a82a260dab094c64fd50f393d31ec7b55cfe6152d79bfbea3cbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
